{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "import uuid  # Import UUID module\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KomikCast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url, save_path):\n",
    "    if not img_url.startswith(('http://', 'https://')):\n",
    "        print(f\"Invalid URL for {save_path}. Saving a gray placeholder image instead.\")\n",
    "        # Create a 225x225 gray image using PIL as a placeholder\n",
    "        img = Image.new('RGB', (225, 225), color='gray')\n",
    "        img.save(save_path)\n",
    "    else:\n",
    "        try:\n",
    "            # Get the image from the valid URL\n",
    "            get_image = requests.get(img_url)\n",
    "            with open(save_path, 'wb') as file:\n",
    "                file.write(get_image.content)\n",
    "                \n",
    "            print(f\"Image downloaded for {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image for {save_path}: {e}\")\n",
    "            # Save a gray image in case of failure\n",
    "            img = Image.new('RGB', (225, 225), color='gray')\n",
    "            img.save(save_path)\n",
    "            \n",
    "def getdataurl(url):\n",
    "    data = []\n",
    "    response = requests.get(url) # Send a GET request to the URL\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    comics = soup.find_all('div', class_='list-update_item')\n",
    "    for comic in comics:\n",
    "        # raw data comics\n",
    "        title = comic.find('h3', class_='title').text\n",
    "        img = comic.find('img', class_='ts-post-image').get('src')\n",
    "        raw_rate = comic.find('div', class_='numscore').text.replace(',', '.').replace('..', '.').strip()\n",
    "        rate = float(raw_rate) if re.match(r'^\\d+(\\.\\d+)?$', raw_rate) else 0.0\n",
    "\n",
    "        type = comic.find('span', class_='type').text\n",
    "        \n",
    "        # sub-comics to get details\n",
    "        soup = BeautifulSoup((requests.get(comic.find('a').get('href'))).content, 'html.parser')\n",
    "        raw_description = soup.find('div', class_=\"komik_info-description-sinopsis\").text.strip()\n",
    "        description = re.sub(r'[^a-zA-Z0-9\\s]', '', raw_description)\n",
    "        alt_title = soup.find('span',class_='komik_info-content-native').text.strip()\n",
    "        released = (soup.find('span', class_='komik_info-content-info-release').text.strip()).replace('Released:', '').strip() \n",
    "        author = (soup.find('span', class_='komik_info-content-info').text.strip()).replace('Author:', '').strip()\n",
    "        \n",
    "        # Extract genres\n",
    "        raw_genre = soup.find('span', class_='komik_info-content-genre')\n",
    "        if raw_genre:\n",
    "            genres = [a.text.strip() for a in raw_genre.find_all('a')]\n",
    "            genre = ', '.join(genres)\n",
    "        else:\n",
    "            genre = \"\"\n",
    "        \n",
    "        uuid_data = str(uuid.uuid4())\n",
    "        data.append({\n",
    "                'id': uuid_data,\n",
    "                'title': title,\n",
    "                'alt_title': alt_title,\n",
    "                'type': type,\n",
    "                'description': description,\n",
    "                'genre': genre,\n",
    "                'author':author,\n",
    "                'artist':\"-\",\n",
    "                'rate': rate,\n",
    "                'image': img,\n",
    "                'released': released,\n",
    "            })\n",
    "        \n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "            \n",
    "        download_image(img, f'image/{uuid_data}.jpg')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://komikcast.cz/daftar-komik'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    json = []\n",
    "    url += \"/page\"\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') # Parse the HTML content of the page\n",
    "    comics = soup.find_all('div', class_='list-update_item')\n",
    "    last_page = int((soup.find_all('a', class_='page-numbers')[-2]).text) + 1\n",
    "    \n",
    "    indexNum = 0\n",
    "    last_page = 400\n",
    "    \n",
    "    for start in range(indexNum, last_page, 100):\n",
    "        end = min(start + 100, last_page)\n",
    "        json_data = []\n",
    "        for i in range(start, end):\n",
    "            url_page = url + \"/\" + str(i)\n",
    "            getdata = getdataurl(url_page)\n",
    "            json_data.extend(getdata)\n",
    "        \n",
    "        df = pd.DataFrame(json_data)\n",
    "        \n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "        \n",
    "        df.to_csv(f'data/komikcast-{end}.csv', index=False, encoding='utf-8')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
